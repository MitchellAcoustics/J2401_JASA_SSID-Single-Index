<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">

<head>

<meta charset="utf-8" />
<meta name="generator" content="quarto-1.4.553" />

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />

<meta name="author" content="Andrew Mitchell" />
<meta name="author" content="Francesco Aletta" />
<meta name="author" content="Tin Oberman" />
<meta name="author" content="Jian Kang" />
<meta name="dcterms.date" content="2024-04-26" />
<meta name="keywords" content="keyword1, keyword2" />

<title>SPI - Defining bespoke and archetypal context-dependent Soundscape Perception Indices</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<!-- htmldependencies:E3FAD763 -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous" data-relocate-top="true"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <div id="quarto-toc-target"></div>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default toc-left">
  <div class="quarto-title-banner">
    <div class="quarto-title column-body">
      <h1 class="title">SPI - Defining bespoke and archetypal context-dependent Soundscape Perception Indices</h1>
          </div>

    
    <div class="quarto-title-meta-container">
      <div class="quarto-title-meta-column-start">
            <div class="quarto-title-meta-author">
          <div class="quarto-title-meta-heading">Authors</div>
          <div class="quarto-title-meta-heading">Affiliation</div>
          
                <div class="quarto-title-meta-contents">
            <p class="author">Andrew Mitchell <a href="mailto:andrew.mitchell.18@ucl.ac.uk" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
          </div>
                <div class="quarto-title-meta-contents">
                    <p class="affiliation">
                        University College London
                      </p>
                  </div>
                      <div class="quarto-title-meta-contents">
            <p class="author">Francesco Aletta <a href="mailto:f.aletta@ucl.ac.uk" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
          </div>
                <div class="quarto-title-meta-contents">
                    <p class="affiliation">
                        University College London
                      </p>
                  </div>
                      <div class="quarto-title-meta-contents">
            <p class="author">Tin Oberman <a href="mailto:t.oberman@ucl.ac.uk" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
          </div>
                <div class="quarto-title-meta-contents">
                    <p class="affiliation">
                        University College London
                      </p>
                  </div>
                      <div class="quarto-title-meta-contents">
            <p class="author">Jian Kang <a href="mailto:j.kang@ucl.ac.uk" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
          </div>
                    </div>
        
        <div class="quarto-title-meta">

                      
                <div>
            <div class="quarto-title-meta-heading">Published</div>
            <div class="quarto-title-meta-contents">
              <p class="date">April 26, 2024</p>
            </div>
          </div>
          
                
              </div>
      </div>
      <div class="quarto-title-meta-column-end quarto-other-formats-target">
      </div>
    </div>

    <div>
      <div class="abstract">
        <div class="block-title">Abstract</div>
        <p>The soundscape approach provides a basis for considering the holistic perception of sound environments, in context. While steady advancements have been made in methods for assessment and analysis, a gap exists for comparing soundscapes and quantifying improvements in the multi-dimensional perception of a soundscape. To this end, there is a need for the creation of single value indices to compare soundscape quality which incorporate context, aural diversity, and specific design goals for a given application. Just as a variety of decibel-based indices have been developed for various purposes (e.g. LAeq, LCeq, L90, Lden, etc.), the soundscape approach requires the ability to create novel indices for different uses, but which share a common language and understanding. We therefore propose a unified framework for creating both bespoke and stan dardised single index measures of soundscape perception based on the soundscape circumplex model, allowing for new metrics to be defined in the future. The implementation of this framework is demonstrated through the creation of a public spaced typology-based index using data collected under the SSID Protocol, which was designed specifically for the purpose of defining soundscape indices. Indices developed under this framework can enable a broader and more efficient application of the soundscape approach.</p>
      </div>
    </div>

    <div>
      <div class="keywords">
        <div class="block-title">Keywords</div>
        <p>keyword1, keyword2</p>
      </div>
    </div>

    <div class="quarto-other-links-text-target">
    </div>  </div>
</header>

  

<nav id="TOC" role="doc-toc">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction"><span class="header-section-number">1</span> Introduction</a>
  <ul>
  <li><a href="#the-need-for-soundscape-indices" id="toc-the-need-for-soundscape-indices"><span class="header-section-number">1.1</span> The need for Soundscape Indices</a></li>
  <li><a href="#motivations-goals" id="toc-motivations-goals"><span class="header-section-number">1.2</span> Motivations &amp; Goals</a></li>
  </ul></li>
  <li><a href="#methodology" id="toc-methodology"><span class="header-section-number">2</span> Methodology</a>
  <ul>
  <li><a href="#dataset" id="toc-dataset"><span class="header-section-number">2.1</span> Dataset</a></li>
  <li><a href="#soundscape-circumplex-projection" id="toc-soundscape-circumplex-projection"><span class="header-section-number">2.2</span> Soundscape Circumplex &amp; Projection</a></li>
  <li><a href="#sec-circumplex-distribution" id="toc-sec-circumplex-distribution"><span class="header-section-number">2.3</span> Circumplex Distribution</a></li>
  <li><a href="#defining-the-spi-framework" id="toc-defining-the-spi-framework"><span class="header-section-number">2.4</span> Defining the SPI Framework</a>
  <ul>
  <li><a href="#defining-a-target" id="toc-defining-a-target"><span class="header-section-number">2.4.1</span> Defining a Target</a></li>
  <li><a href="#distance-metric" id="toc-distance-metric"><span class="header-section-number">2.4.2</span> Distance Metric</a></li>
  <li><a href="#targets" id="toc-targets"><span class="header-section-number">2.4.3</span> Targets</a></li>
  <li><a href="#data-source" id="toc-data-source"><span class="header-section-number">2.4.4</span> Data Source</a></li>
  </ul></li>
  <li><a href="#deriving-a-target" id="toc-deriving-a-target"><span class="header-section-number">2.5</span> Deriving a target</a></li>
  </ul></li>
  </ul>
</nav>
<section id="introduction" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Introduction</h1>
<p>A common aim for implementing soundscape perception research in practice is to compare the quality of different soundscapes. Often (but not always) the goal is to identify a ‘good’ soundscape compared to a ‘bad’ soundscape. However, this presents several challenges:</p>
<ul>
<li>What makes a soundscape good or bad is highly contextual;</li>
<li>On what metric should the quality rating be based?</li>
<li>How can we deal with different requirements and definitions of how a soundscape should be perceived?</li>
</ul>
<p>In many cases, the ultimate aim is to be able to rank soundscapes based on their quality. However, any ranking metric should be flexible and be able to handle a variety of contexts and definitions of what a good soundscape is for a given purpose. To address this, we will propose the Soundscape Perception Index (SPI) framework, a flexible method for defining single value indices of soundscape quality based on distributions within the Soundscape Circumplex Model (SCM) <span class="citation" data-cites="Axelsson2010principal">Mitchell et al. (<a href="#ref-Mitchell2022How" role="doc-biblioref">2022</a>)</span>. This paper will demonstrate the SPI framework and test whether it is capable of both scoring soundscape quality and generating consistent rankings of soundscapes across different contexts.</p>
<!-- The EU Green Paper on Future Noise Policy indicates that 80 million EU citizens are suffering from unacceptable environmental noise levels, according to the WHO recommendation [@Berglund1999Guidelines] and the social cost of transport noise is 0.2-2% of total GDP. The publication of the EU Directive Relating to the Assessment and Management of Environmental Noise (END) [@EuropeanUnion2002Directive] in 2002 has led to major actions across Europe, with reducing noise levels as the focus, for which billions of Euros are being spent. However, it is widely recognised that solely reducing sound level is not always feasible or cost-effective, and more importantly, with only ~30% of environmental noise annoyance depending on facets of parameters such as acoustic energy [@Guski1997Psychological], sound level reduction will not necessarily lead to improved quality of life. -->
<!-- Soundscape design, separate from noise control engineering, is about the relationships between human physiology, perception, the sound environment, and its social/cultural context [@Kang2006Urban]. Soundscape research represents a paradigm shift in that it combines physical, social, and psychological approaches and considers environmental sounds as a 'resource' rather than 'waste' [@Kang2016Soundscape] relating to perceptual constructs rather than just physical phenomena. However, the current research is still at the stage of describing and identifying the problems and tends to be fragmented and focussed on only special cases e.g. subjective evaluations of soundscapes for residential areas [@SchulteFortkamp2013Introduction]. In the movement from noise control to soundscape creation [@Aletta2015Soundscape], a vital step is the standardisation of methods to assess soundscape quality. -->
<p>In <span class="citation" data-cites="Aletta2016Soundscape">Aletta et al. (<a href="#ref-Aletta2016Soundscape" role="doc-biblioref">2016</a>)</span>, the authors defined a framework for categorising the components of a soundscape assessment. They define three aspects: soundscape descriptors, soundscape indicators, and soundscape indices. Soundscape descriptors are defined as ‘measures of how people perceive the acoustic environment’ and soundscape indicators as ‘measures used to predict the value of a soundscape descriptor’. Soundscape indices can then be defined as ‘single value scales derived from either descriptors or indicators that allow for comparison across soundscapes’ <span class="citation" data-cites="Kang2019Towards">(<a href="#ref-Kang2019Towards" role="doc-biblioref">Kang et al., 2019</a>)</span>.</p>
<!-- End Thesis -->
<p>Soundscape indicators refer to measurable aspects or attributes of a soundscape, such as loudness, tonal characteristics, or spectral content, which can be quantified through objective measurements or signal processing techniques. In contrast, soundscape descriptors are qualitative representations of the perceived characteristics of a soundscape, often derived from listener evaluations, subjective assessments, or semantic differential scales <span class="citation" data-cites="ISO12913Part2">(<a href="#ref-ISO12913Part2" role="doc-biblioref">ISO/TS 12913-2:2018, 2018</a>)</span>.</p>
<p>Indices, the primary focus of this article, are single numerical values that combine multiple indicators or descriptors to provide a comprehensive representation of the overall soundscape perception. These indices serve as powerful tools for quantifying and comparing soundscapes, enabling decision-makers and stakeholders to assess the impact of interventions, monitor changes over time, and prioritize areas for improvement.</p>
<!-- TODO: Should rephrase the following paragraphs -->
<p>The Decibel (dB) is the earliest and most commonly used scientific index measuring sound level. To represent the overall level of sound with a single value on one scale, as the Decibel index does, is often desirable. For this purpose, a number of different values representing sounds at various frequencies must be combined. Several frequency weighting networks have been developed since the 1930s, considering typical human responses to sound based on equal-loudness-level contours <span class="citation" data-cites="Fletcher1933Loudness">(<a href="#ref-Fletcher1933Loudness" role="doc-biblioref">Fletcher &amp; Munson, 1933</a>)</span> and, among them, the A-weighting network, with resultant decibel values called dBA, has been commonly used in almost all the national/international regulations <span class="citation" data-cites="Kryter1970Effects">(<a href="#ref-Kryter1970Effects" role="doc-biblioref">Kryter, 1970</a>)</span>. However, there have been numerous criticisms on its effectiveness <span class="citation" data-cites="Parmanen2007weighted">(<a href="#ref-Parmanen2007weighted" role="doc-biblioref">Parmanen, 2007</a>)</span> as the correlations between dBA and perceived sound quality (e.g. noise annoyance) are often low <span class="citation" data-cites="Hellman1987Why">(<a href="#ref-Hellman1987Why" role="doc-biblioref">Hellman &amp; Zwicker, 1987</a>)</span>.</p>
<p>Another set of indices is psychoacoustic magnitudes, including loudness, fluctuation strength or roughness, sharpness, and pitch strength, development with sound quality studies of industrial products since the 1980’s <span class="citation" data-cites="Zwicker2007Psychoacoustics">(<a href="#ref-Zwicker2007Psychoacoustics" role="doc-biblioref">Zwicker &amp; Fastl, 2007</a>)</span>. These emerged when it was conceived that acoustic emissions had further characteristics than just level <span class="citation" data-cites="Blauert1997Sound">(<a href="#ref-Blauert1997Sound" role="doc-biblioref">Blauert &amp; Jekosch, 1997</a>)</span>. But while psychoacoustic magnitudes have been proved to be successful for the assessment of product sound quality, in the field of environmental acoustics, their applicability has been limited <span class="citation" data-cites="Fastl2006Psychoacoustic">(<a href="#ref-Fastl2006Psychoacoustic" role="doc-biblioref">Fastl, 2006</a>)</span>, since a significant feature of environmental acoustics is that there are multiple/dynamic sound sources.</p>
<p>Attendant with the transition from a noise reduction to soundscape paradigm is an urgent need for developing appropriate indices for soundscape, rather than continuously using dBA <span class="citation" data-cites="Andringa2013Positioning">(<a href="#ref-Andringa2013Positioning" role="doc-biblioref">Andringa et al., 2013</a>)</span>.</p>
<section id="the-need-for-soundscape-indices" class="level2" data-number="1.1">
<h2 data-number="1.1"><span class="header-section-number">1.1</span> The need for Soundscape Indices</h2>
<!-- From Thesis -->
<p>Soundscape studies strive to understand the perception of a sound environment, in context, including acoustic, (non-acoustic) environmental, contextual, and personal factors. These factors combine together to form a person’s soundscape perception in complex interacting ways <span class="citation" data-cites="Berglund2006Soundscape">(<a href="#ref-Berglund2006Soundscape" role="doc-biblioref">Berglund &amp; Nilsson, 2006</a>)</span>. Humans and soundscapes have a dynamic bidirectional relationship - while humans and their behaviour directly influence their soundscape, humans and their behaviour are in turn influenced by their soundscape <span class="citation" data-cites="Erfanian2019Psychophysiological">(<a href="#ref-Erfanian2019Psychophysiological" role="doc-biblioref">Erfanian et al., 2019</a>)</span>.</p>
<p>When applied to urban sound and specifically to noise pollution, the soundscape approach introduces three key considerations beyond traditional noise control methods:</p>
<ol type="1">
<li>considering all aspects of the environment which may influence perception, not just the sound level and spectral content;</li>
<li>an increased and integrated consideration of the varying impacts which different sound sources and sonic characteristics have on perception; and</li>
<li>a consideration of both the positive and negative dimensions of soundscape perception.</li>
</ol>
<p>This approach can enable better outcomes by identifying positive soundscapes (in line with the END’s mandate to `preserve environmental noise quality where it is good’ <span class="citation" data-cites="EuropeanUnion2002Directive">(<a href="#ref-EuropeanUnion2002Directive" role="doc-biblioref">European Union, 2002</a>)</span>), better identify specific sources of noise which impact soundscape quality and pinpoint the characteristics which may need to be decreased, and illuminate alternative methods which could be introduced to improve a soundscape where a reduction of noise is impractical <span class="citation" data-cites="Fiebig2018Does Kang2018Impact">(<a href="#ref-Fiebig2018Does" role="doc-biblioref">Fiebig, 2018</a>; <a href="#ref-Kang2018Impact" role="doc-biblioref">Kang &amp; Aletta, 2018</a>)</span>. These can all lead to more opportunities to truly improve a space by identifying the causes of positive soundscapes, while also potentially decreasing the costs of noise mitigation by offering more targeted techniques and alternative approaches.</p>
<p>The traditional focus on noise levels alone fails to capture the complexity of soundscape perception, which encompasses a multitude of factors beyond mere sound pressure levels. Factors such as the presence of natural or human-made sounds, their temporal patterns, and the overall contextual meaning ascribed to these sounds all contribute to the holistic perception of a soundscape. Consequently, there is a pressing need for the development of robust indices that can encapsulate this multi-dimensional nature of soundscape perception, enabling comparative evaluations and informing targeted interventions to enhance the overall quality of acoustic environments <span class="citation" data-cites="Chen2023Developing">(<a href="#ref-Chen2023Developing" role="doc-biblioref">Chen et al., 2023</a>)</span>.</p>
<!-- End Thesis -->
<p>Across both the visual and the auditory domain, research has suggested that a disconnect exists between the physical metrics used to describe urban environments and how they are perceived <span class="citation" data-cites="Kruize2019Exploring Yang2005Acoustic">(<a href="#ref-Kruize2019Exploring" role="doc-biblioref">Kruize et al., 2019</a>; <a href="#ref-Yang2005Acoustic" role="doc-biblioref">Yang &amp; Kang, 2005</a>)</span>. In addition, this disconnect can be extended further into how these environments influence the health and well-being of their users. To gain a better understanding of these spaces and their immpacts on people who work and live in cities, we must create assessment methods and metrics which go beyond merely characterising the physical environment and instead translate through the user’s perception <span class="citation" data-cites="Mitchell2022Predictive">(<a href="#ref-Mitchell2022Predictive" role="doc-biblioref">Mitchell, 2022</a>)</span>.</p>
<!-- # Literature Review

[@Grinfeder2022What]

## Existing 'Soundscape Indices'

While the field of soundscape research has witnessed substantial progress, the development of standardized indices for evaluating and comparing soundscapes across diverse contexts has been relatively limited. Existing indices can be broadly seen as arising from two domains: soundscape ecology and soundscape perception. It is worth reviewing these indices to highlight how the framework proposed here is fundamentally different in both concept and aim.

### Soundscape Ecology

Within the realm of soundscape ecology, indices such as the Acoustic Diversity Index (ADI) and Frequency-dependenty Acoustic Diversity Index (FADI) [@Xu2023frequency] have been developed to quantify the diversity and complexity of acoustic signals within a given soundscape. These indices are particularly useful in ecological studies, providing insights into the richness and diversity of biophonic (natural) and anthrophonic (human-made) sound sources.

<mark>***Add additional information on ADI, FADI, NDSI, etc.***</mark>

However, while these indices contribute valuable insights into the ecological aspects of soundscapes, they do not directly address the perceptual dimensions that are central to the soundscape approach [@SchulteFortkamp2023Soundscapes]. The multi-dimensional nature of soundscape perception, encompassing factors such as pleasantness, eventfulness, and familiarity, necessitates a more comprehensive and context-sensitive approach.

### Soundscape Perception

In the domain of soundscape perception, the Green Soundscape Index (GSI) [@Kogan2018Green] has emerged as a notable attempt to quantify the perceived quality of soundscapes, particularly in urban environments. This index incorporates factors such as the presence and levels of natural sounds, human-made sounds, and their respective contributions to the overall soundscape perception.

The GSI is the ratio of the perceived extent of natural sounds (PNS) to the perceived extent of traffic noise (PTN):

$$
GSI = \frac{<PNS>}{<PTN>}
$$

The GSI is noted to range between 1/5 and 5, with several ranges of values given which correspond to general categories of the perceived dominance of traffic noise.

While GSI represents a commendable effort to bridge the gap between objective measurements and subjective perceptions, it remains limited in its ability to capture the full complexity of soundscape perception across diverse contexts. The intricate interplay between various sound sources, their temporal patterns, and the specific context in which they are experienced necessitates a more flexible and adaptable approach to index development. -->
</section>
<section id="motivations-goals" class="level2" data-number="1.2">
<h2 data-number="1.2"><span class="header-section-number">1.2</span> Motivations &amp; Goals</h2>
<p>The primary motivation behind the development of the Soundscape Perception Indices (SPIs) framework stems from the need to address the existing gap in quantifying and comparing soundscape quality across diverse contexts and applications. By creating a unified framework for defining these indices, the aim is to facilitate a broader and more efficient application of the soundscape approach in various domains, such as urban planning, environmental management, acoustic design, and policy development.</p>
<p>The overarching aim of this framework is to empower stakeholders, decision-makers, and researchers with the ability to create tailored indices that align with their specific objectives and design goals, while simultaneously enabling cross-comparisons and benchmarking against empirically-defined soundscape archetypes. This dual approach not only acknowledges the context-dependent nature of soundscape perception but also fosters a common language and understanding, facilitating knowledge sharing and collaborative efforts within the field.</p>
<p><em>Ranking</em> - The ability to rank soundscapes based on their quality is a key goal of the SPI framework. This ranking can be used to compare soundscapes across different contexts, identify areas for improvement, and prioritize interventions accordingly.</p>
<p><em>Standardisation</em> - The SPI framework aims to provide a standardized approach for defining and calculating soundscape indices, ensuring consistency and comparability across different applications and domains. This standardization enables the development of best practices and facilitates knowledge exchange within the field.</p>
</section>
</section>
<section id="methodology" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Methodology</h1>
<p>An index framework called the Soundscape Perception Indices (SPI) is defined here as the agreement between an observed or modelled soundscape perception distribution and a target soundscape perception distribution. We refer to this as an index framework rather than a single index, as the SPI can be tailored to specific contexts and applications by defining a range of target distributions. A single index is thus created for each target distribution, Throughout this manuscript we will discuss several methods of applications for SPI indices.</p>
<section id="dataset" class="level2" data-number="2.1">
<h2 data-number="2.1"><span class="header-section-number">2.1</span> Dataset</h2>
<p>We use the data contained in the International Soundscape Database (ISD) (Mitchell et al., 2021a), which includes 1300+ individual responses collected across 13 locations in London and Venice, according to the SSID Protocol <span class="citation" data-cites="Mitchell2020Soundscape">Mitchell et al. (<a href="#ref-Mitchell2020Soundscape" role="doc-biblioref">2020</a>)</span>.</p>
</section>
<section id="soundscape-circumplex-projection" class="level2" data-number="2.2">
<h2 data-number="2.2"><span class="header-section-number">2.2</span> Soundscape Circumplex &amp; Projection</h2>
<p>SPI is grounded in the soundscape circumplex model <span class="citation" data-cites="Axelsson2010principal Axelsson2012Swedish">(<a href="#ref-Axelsson2010principal" role="doc-biblioref">Ö. Axelsson et al., 2010</a>; <a href="#ref-Axelsson2012Swedish" role="doc-biblioref">Östen Axelsson et al., 2012</a>)</span>, a robust theoretical foundation for understanding and representing the multi-dimensional nature of soundscape perception. The reason for grounding the SPI in the soundscape circumplex is that we have observed this model (and its corresponding PAQs) to become the most prevalent assessment model in soundscape literature <span class="citation" data-cites="Aletta2023Adoption">(<a href="#ref-Aletta2023Adoption" role="doc-biblioref">Aletta &amp; Torresin, 2023</a>)</span>.</p>
<!-- From Thesis -->
<p>Method A is built on a series of descriptors referred to as the Perceived Affective Quality (PAQ), proposed by <span class="citation" data-cites="Axelsson2010principal">(<a href="#ref-Axelsson2010principal" role="doc-biblioref">Ö. Axelsson et al., 2010</a>)</span>. These PAQs are based on the pleasantness-activity paradigm present in research on emotions and environmental psychology, in particular Russell’s circumplex model of affect <span class="citation" data-cites="Russell1980circumplex">(<a href="#ref-Russell1980circumplex" role="doc-biblioref">Russell, 1980</a>)</span>. As summarised by Axelsson: “Russell’s model identifies two dimensions related to the perceived pleasantness of environments and how activating or arousing the environment is.”</p>
<p>One benefit of the circumplex model is that, as a whole, it encapsulates several of the other proposed soundscape descriptors - in particular, annoyance, pleasantness, tranquility, and possibly restorativeness <span class="citation" data-cites="Aletta2016Soundscape">(<a href="#ref-Aletta2016Soundscape" role="doc-biblioref">Aletta et al., 2016</a>)</span>. According to <span class="citation" data-cites="Axelsson2015How">Ö. Axelsson (<a href="#ref-Axelsson2015How" role="doc-biblioref">2015</a>)</span>, the two-dimensional circumplex model of perceived affective quality provides the most comprehensive information for soundscape assessment. It is also possible that the overall soundscape quality could itself be derived from the pleasant-eventful scores derived for a soundscape. The circumplex also lends itself well to questionnaire-based methods of data collection, as proposed in <span class="citation" data-cites="ISO12913Part2">ISO/TS 12913-2:2018 (<a href="#ref-ISO12913Part2" role="doc-biblioref">2018</a>)</span>. In contrast to methods such as soundwalks, interviews, and lab experiments, in-situ questionnaires are able to provide the quality and amount of data which is necessary for statistical modelling. Combined, these factors make the circumplex most appropriate for a single index as it provides a comprehensive summary of soundscape perception.</p>
<p>To move the 8-item PAQ responses into the 2-dimensional circumplex space, we use the projection method first presented in ISO 12913-3:2018. This projection method and its associated formulae were recently updated further in <span class="citation" data-cites="Mitchell2023Testing">Mitchell &amp; Aletta (<a href="#ref-Mitchell2023Testing" role="doc-biblioref">2023</a>)</span> to include a correction for the language in which the survey was conducted. The formulae are as follows:</p>
<p><span class="math display">\[
P_{ISO} = \frac{1}{\lambda_{Pl}} \sum_{i=1}^{8} \cos \theta_i \cdot \sigma_i
\]</span></p>
<p><span class="math display">\[
E_{ISO} = \frac{1}{\lambda_{Ev}} \sum_{i=1}^{8} \sin \theta_i \cdot \sigma_i
\]</span></p>
<p><mark>Revise</mark></p>
<p><span class="math display">\[
\lambda_{Pl} = \frac{\rho}{2} \sum_{i=1}^{8} |\cos \theta_i|
\]</span></p>
<p><span class="math display">\[
\lambda_{Ev} = \frac{\rho}{2} \sum_{i=1}^{8} |\sin \theta_i|
\]</span></p>
<p>Using the angles derived in <span class="citation" data-cites="Mitchell2023Testing">Mitchell &amp; Aletta (<a href="#ref-Mitchell2023Testing" role="doc-biblioref">2023</a>)</span>, the following table is used to convert the angles into the ISO 12913-3:2018 circumplex space:</p>
<div class="quarto-embed-nb-cell" data-notebook="/Users/mitch/Library/CloudStorage/OneDrive-UniversityCollegeLondon/_Fellowship/Papers - Drafts/J2401_JASA_SSID-Single-Index/notebooks/SingleIndex-Code.ipynb" data-notebook-title="SPI - Defining bespoke and archetypal context-dependent Soundscape Perception Indices" data-notebook-cellId="cell-tbl-lang-angles">
<div class="cell" data-execution_count="6">
<div id="tbl-lang-angles" class="cell quarto-float" data-execution_count="6">
<figure class="quarto-float quarto-float-tbl">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-lang-angles-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table 1: Language-specific angles for projection into the ISO 12913-3:2018 circumplex space.
</figcaption>
<div aria-describedby="tbl-lang-angles-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display" data-execution_count="6">
<div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>

<table class="dataframe do-not-create-environment cell" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header" style="text-align: right;">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">PAQ1</th>
<th data-quarto-table-cell-role="th">PAQ2</th>
<th data-quarto-table-cell-role="th">PAQ3</th>
<th data-quarto-table-cell-role="th">PAQ4</th>
<th data-quarto-table-cell-role="th">PAQ5</th>
<th data-quarto-table-cell-role="th">PAQ6</th>
<th data-quarto-table-cell-role="th">PAQ7</th>
<th data-quarto-table-cell-role="th">PAQ8</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">eng</td>
<td>0</td>
<td>46</td>
<td>94</td>
<td>138</td>
<td>177</td>
<td>241</td>
<td>275</td>
<td>340</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">arb</td>
<td>0</td>
<td>36</td>
<td>45</td>
<td>135</td>
<td>167</td>
<td>201</td>
<td>242</td>
<td>308</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">cmn</td>
<td>0</td>
<td>18</td>
<td>38</td>
<td>154</td>
<td>171</td>
<td>196</td>
<td>217</td>
<td>318</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">hrv</td>
<td>0</td>
<td>84</td>
<td>93</td>
<td>160</td>
<td>173</td>
<td>243</td>
<td>273</td>
<td>354</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">nld</td>
<td>0</td>
<td>43</td>
<td>111</td>
<td>125</td>
<td>174</td>
<td>257</td>
<td>307</td>
<td>341</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">deu</td>
<td>0</td>
<td>64</td>
<td>97</td>
<td>132</td>
<td>182</td>
<td>254</td>
<td>282</td>
<td>336</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">ell</td>
<td>0</td>
<td>72</td>
<td>86</td>
<td>133</td>
<td>161</td>
<td>233</td>
<td>267</td>
<td>328</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">ind</td>
<td>0</td>
<td>53</td>
<td>104</td>
<td>123</td>
<td>139</td>
<td>202</td>
<td>284</td>
<td>308</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">ita</td>
<td>0</td>
<td>57</td>
<td>104</td>
<td>143</td>
<td>170</td>
<td>274</td>
<td>285</td>
<td>336</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">spa</td>
<td>0</td>
<td>41</td>
<td>103</td>
<td>147</td>
<td>174</td>
<td>238</td>
<td>279</td>
<td>332</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">swe</td>
<td>0</td>
<td>66</td>
<td>87</td>
<td>146</td>
<td>175</td>
<td>249</td>
<td>275</td>
<td>335</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">tur</td>
<td>0</td>
<td>55</td>
<td>97</td>
<td>106</td>
<td>157</td>
<td>254</td>
<td>289</td>
<td>313</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</div>
</figure>
</div>
</div>
</div>
</section>
<section id="sec-circumplex-distribution" class="level2" data-number="2.3">
<h2 data-number="2.3"><span class="header-section-number">2.3</span> Circumplex Distribution</h2>
<p>Once the individual perceptual responses are projected into the circumplex space, the resulting data for each location is treated as a circumplex distribution. The circumplex is defined by two axes: <span class="math inline">\(P_{ISO}\)</span> and <span class="math inline">\(E_{ISO}\)</span>, which are limited to the range <span class="math inline">\([-1, +1]\)</span>. Typically, data in the soundscape circumplex is treated as a combination of two independent normal distributions, one for each axis <span class="citation" data-cites="Mitchell2022How">(<a href="#ref-Mitchell2022How" role="doc-biblioref">Mitchell et al., 2022</a>)</span>. In some applications, this approach is sufficient for capturing the distribution of soundscape perception, however the method for calculating the SPI requires a more precise approach. The independent normal distribution approach relies on three key assumptions:</p>
<ol type="1">
<li>The two axes are normally distributed.</li>
<li>The two axes are independent of each other.</li>
<li>The two axes are symmetrically distributed.</li>
</ol>
<p>While the first assumption is generally valid, the second and third assumptions are not always met in practice. In particular, the distribution of soundscape perception responses in the circumplex is often characterised by a high degree of skewness, which can lead to inaccuracies in the calculation of the SPI. Soundscape circumplex distributions are most appropriately described as a bivariate skew-normal distribution <span class="citation" data-cites="Azzalini2005Skew">(<a href="#ref-Azzalini2005Skew" role="doc-biblioref">Adelchi Azzalini, 2005</a>)</span> which accurately reflects the relationship between the two dimensions of the circumplex and the fact that real-world perceptual distributions have been consistently observed to not be strictly symmetric.</p>
<p>The skew-normal distribution is defined by three parameters: location (<span class="math inline">\(\mu\)</span>), scale (<span class="math inline">\(\sigma\)</span>), and shape (<span class="math inline">\(\alpha\)</span>). The location parameter defines the centre of the distribution, the scale parameter defines the spread of the distribution and the shape parameter defines the skew of the distribution. The one-dimensional skew-normal distribution is defined as <span class="citation" data-cites="Azzalini1996Multivariate">(<a href="#ref-Azzalini1996Multivariate" role="doc-biblioref">A. Azzalini &amp; Valle, 1996</a>)</span>:</p>
<p><span class="math display">\[
\phi(z; \alpha) = 2 \phi(z) \Phi(\alpha z) \quad \text{for} \quad z \in \mathbb{R}
\]</span></p>
<p>where <span class="math inline">\(\phi\)</span> and <span class="math inline">\(\Phi\)</span> are the standard normal probability density function and distribution function, respectively, and <span class="math inline">\(\alpha\)</span> is a shape variable which regulates the skewness. The distribution reduces to a standard normal density when <span class="math inline">\(\alpha = 0\)</span>. The bivariate skew-normal distribution extends this concept to two dimensions, allowing for the modelling of asymmetric and skewed distributions in a two-dimensional space such as the soundscape circumplex. The multivariate skew-normal (MSN) distribution including scale and location parameters is given by combining the normal density and distribution functions <span class="citation" data-cites="Azzalini1999Statistical">(<a href="#ref-Azzalini1999Statistical" role="doc-biblioref">A. Azzalini &amp; Capitanio, 1999</a>)</span>:</p>
<p><span class="math display">\[
Y = 2 \phi_k (y-\xi; \Omega) \Phi\{\alpha^T\omega^{-1}(y-\xi)\}
\]</span></p>
<p>where <span class="math inline">\(\phi_k\)</span> is the <em>k</em>-dimensional normal density with location <span class="math inline">\(\xi\)</span>, shape <span class="math inline">\(\alpha\)</span>, and covariance matrix <span class="math inline">\(\Omega\)</span>. <span class="math inline">\(\Phi \{ \dot \}\)</span> is the normal distribution function and <span class="math inline">\(\alpha\)</span> is a <em>k</em>-dimensional shape vector. When <span class="math inline">\(\alpha = 0\)</span>, <span class="math inline">\(Y\)</span> reduces to the standard multivariate normal <span class="math inline">\(N_k(\xi, \Omega)\)</span> density. A circumplex distribution can therefore be parameterised with a 2x2 covariance matrix <span class="math inline">\(\Omega\)</span>, a 2x1 location vector <span class="math inline">\(\xi\)</span>, and a 2x1 shape vector <span class="math inline">\(\alpha\)</span>, written as:</p>
<p><span class="math display">\[
Y \sim MSN (\xi, \Omega, \alpha)
\]</span></p>
<p>By fitting an MSN distribution to the soundscape perception responses, it becomes possible to accurately capture the asymmetry and skewness of the distribution. A bivariate skew-normal distribution can be summarised as a set of these three parameters. Once parameterised, the distribution can then be sampled from to generate a synthetic distribution of soundscape perception responses. A demonstration of the results of this process is shown in <a href="#fig-dist-example" class="quarto-xref">Figure 1</a> using data from a single location included in the ISD.</p>
<div class="quarto-embed-nb-cell" data-notebook="/Users/mitch/Library/CloudStorage/OneDrive-UniversityCollegeLondon/_Fellowship/Papers - Drafts/J2401_JASA_SSID-Single-Index/notebooks/SingleIndex-Code.ipynb" data-notebook-title="SPI - Defining bespoke and archetypal context-dependent Soundscape Perception Indices" data-notebook-cellId="cell-fig-dist-example">
<div id="cell-fig-dist-example" class="cell" data-ExecuteTime="{&quot;end_time&quot;:&quot;2024-02-02T22:36:24.997380Z&quot;,&quot;start_time&quot;:&quot;2024-02-02T22:36:22.947404Z&quot;}" data-execution_count="12">
<div class="cell-output cell-output-display">
<div id="fig-dist-example" class="quarto-figure quarto-figure-center quarto-float">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-dist-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="index_files/figure-html/notebooks-SingleIndex-Code-fig-dist-example-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" data-glightbox="description: .lightbox-desc-1"><img src="index_files/figure-html/notebooks-SingleIndex-Code-fig-dist-example-output-1.png" class="img-fluid" /></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-dist-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 1: Example of fitting and sampling from a multivariate skew-normal distribution for data from the Piazza San Marco location.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>The parameters for the distribution shown in <a href="#fig-dist-example" class="quarto-xref">Figure 1</a> are given by:</p>
<p><span class="math display">\[
MSN([0.065, 0.679], \begin{bmatrix} 0.149 &amp; -0.064 \\ -0.064 &amp; 0.101 \end{bmatrix}, [0.791, -0.767])
\]</span></p>
<!-- #### Direct and Centred parameters -->
<!-- ## Data -->
</section>
<section id="defining-the-spi-framework" class="level2" data-number="2.4">
<h2 data-number="2.4"><span class="header-section-number">2.4</span> Defining the SPI Framework</h2>
<p>The Soundscape Perception Indices (SPI) framework is centred around the concept of quantifying the distance between a test distribution of interest and the desired target distribution. Its goal is to determine whether a soundscape - whether it be a real-world location, a proposed design, or a hypothetical scenario - aligns with the desired perception of that soundscape. This is achieved by first defining the target distribution, which could represent what is considered to be the ‘ideal’ soundscape perception for a given context or application. The test distribution is then compared to the target distribution using a distance metric, which quantifies the deviation between the two distributions. The resulting distance value serves as the basis for calculating the SPI, with smaller distances indicating a closer alignment between the perceived soundscape and the target soundscape perception.</p>
<p>Although it is expected that the target distribution would usually represent the ideal or goal soundscape perception, it is also possible to define target distributions that represent undesirable or suboptimal soundscape perceptions. For instance, in a soundscape mapping context, it may be beneficial to map and identify chaotic soundscapes across a city in order to better target areas for soundscape interventions. In this case, the target distribution would be set in the chaotic quadrant and a higher SPI would indicate a closer alignment with the target distribution. This flexibility allows the SPI to be applied to a wide range of contexts and applications, enabling the quantification and comparison of soundscape quality across diverse scenarios.</p>
<p>An SPI value therefore does not represent a ‘good’ or ‘bad’ soundscape, but rather a measure of how closely the perceived soundscape aligns with the desired target soundscape perception. This approach allows for the development of bespoke indices tailored to specific design goals and objectives, while also enabling cross-comparisons and benchmarking against empirically-defined soundscape archetypes.</p>
<section id="defining-a-target" class="level3" data-number="2.4.1">
<h3 data-number="2.4.1"><span class="header-section-number">2.4.1</span> Defining a Target</h3>
<p>As introduced in <a href="#sec-circumplex-distribution" class="quarto-xref">Section 2.3</a>, circumplex data follows a bivariate skew-normal distribution which can be parameterised with a set of direct parameters (dp). We therefore define a target distribution as a set of these parameters, which can then be used to generate a synthetic distribution of soundscape perception responses.</p>
</section>
<section id="distance-metric" class="level3" data-number="2.4.2">
<h3 data-number="2.4.2"><span class="header-section-number">2.4.2</span> Distance Metric</h3>
<p>Central to the SPI framework is the concept of a distance metric, which quantifies the deviation of a given soundscape from a desired target soundscape. This distance metric serves as the basis for calculating the SPI value, with smaller distances indicating a closer alignment between the perceived soundscape and the target soundscape perception. The distance between the test and target soundscape distributions is calculated using a two-dimensional Kolmogorov-Smirnov test <span class="citation" data-cites="Fasano1987multidimensional">(<a href="#ref-Fasano1987multidimensional" role="doc-biblioref">Fasano &amp; Franceschini, 1987</a>)</span>.</p>
<!-- Various distance metrics could be employed, ranging from a simple Euclidean distance

It would be possible to define a single target point, rather than an entire target distribution and assess the test distribution's distance from that point using an $R^2$ based on a euclidian distance. However, as noted in @Mitchell2022How, it is important to also consider the spread of the distribution. 

*Discuss different options of distance metrics and approaches* -->
<p>Essentially, we approach this as a problem of (dis)similarity between soundscapes. The distance metric is then proposed to assess how similar two any given soundscapes distributions are within the circumplex. Taken to the extreme, two perfectly matching distributions in the soundscape circumplex would return a 100% SPI value, while two completely dissimilar distributions would return a 0% SPI value. In practical terms, for the former, this will never be achieved in real world scenarios; for the latter, it is also difficult to estimate how low the SPI value could actually go, and it should be considered that the distance may happen in different directions within the circumplex space. For instance, if a distribution for a vibrant soundscape was taken as a reference, a compared soundscape distribution may exhibit low SPI values for being located in the calm, OR monotonous, OR chaotic regions of the model.</p>
</section>
<section id="targets" class="level3" data-number="2.4.3">
<h3 data-number="2.4.3"><span class="header-section-number">2.4.3</span> Targets</h3>
<p>The SPI framework introduces two distinct types of targets: bespoke targets and archetypal targets, each serving a unique purpose in the index development process.</p>
<section id="bespoke-targets" class="level4" data-number="2.4.3.1">
<h4 data-number="2.4.3.1"><span class="header-section-number">2.4.3.1</span> Bespoke Targets</h4>
<p>Bespoke targets are tailor-made for specific projects, reflecting the desired soundscape perception for a particular application. These targets can be defined by stakeholders, designers, policymakers, or decision-makers based on their unique requirements, objectives, and constraints. This flexibility allows the SPI for a specific project to be tailored to the desire of the stakeholders for how that specific soundscape should function. It can also provide a consistent and quantifiable baseline for scenarios like a soundscape design contest wherein a target is specified and provided to all participants in the contest and the winning proposal is the design with the highest SPI score when assessed against that target.</p>
</section>
<section id="archetypal-targets" class="level4" data-number="2.4.3.2">
<h4 data-number="2.4.3.2"><span class="header-section-number">2.4.3.2</span> Archetypal Targets</h4>
<p>In contrast to bespoke targets, archetypal targets represent generalized, widely recognized soundscape archetypes which transcend specific applications or projects. These archetypes serve as reference points and enable comparisons across different domains and use cases. <strong><em>By providing a framework for these archetypes to be defined, they can be…</em></strong></p>
<p>Additionally, archetypal SPIs can be composed of multiple targets.</p>
</section>
</section>
<section id="data-source" class="level3" data-number="2.4.4">
<h3 data-number="2.4.4"><span class="header-section-number">2.4.4</span> Data Source</h3>
<p>The SPI framework is designed to accommodate a wide range of data sources, including both objective measurements and subjective evaluations. This flexibility enables the framework to be applied to diverse contexts and applications, ranging from urban soundscapes to natural environments, public spaces, and indoor settings.</p>
</section>
</section>
<section id="deriving-a-target" class="level2" data-number="2.5">
<h2 data-number="2.5"><span class="header-section-number">2.5</span> Deriving a target</h2>
<div class="hidden" aria-hidden="true">
<span class="glightbox-desc lightbox-desc-1">Figure 1: Example of fitting and sampling from a multivariate skew-normal distribution for data from the Piazza San Marco location.</span>
</div>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list">
<div id="ref-Aletta2023Adoption" class="csl-entry" role="listitem">
Aletta, F., &amp; Torresin, S. (2023). Adoption of <span>ISO/TS</span> 12913-2:2018 protocols for data collection from individuals in soundscape studies: <span>A</span>n overview of the literature. <em>Current Pollution Reports</em>. <a href="https://doi.org/10.1007/s40726-023-00283-6">https://doi.org/10.1007/s40726-023-00283-6</a>
</div>
<div id="ref-Aletta2016Soundscape" class="csl-entry" role="listitem">
Aletta, F., Kang, J., &amp; Axelsson, Östen. (2016). <span class="nocase">Soundscape descriptors and a conceptual framework for developing predictive soundscape models</span>. <em>Landscape and Urban Planning</em>, <em>149</em>, 65–74. <a href="https://doi.org/10.1016/j.landurbplan.2016.02.001">https://doi.org/10.1016/j.landurbplan.2016.02.001</a>
</div>
<div id="ref-Andringa2013Positioning" class="csl-entry" role="listitem">
Andringa, T. C., Weber, M., Payne, S. R., Krijnders, J. D. (Dirkjan), Dixon, M. N., Linden, R. v.d., et al. (2013). Positioning soundscape research and management. <em>The Journal of the Acoustical Society of America</em>, <em>134</em>(4), 2739–2747. <a href="https://doi.org/10.1121/1.4819248">https://doi.org/10.1121/1.4819248</a>
</div>
<div id="ref-Axelsson2015How" class="csl-entry" role="listitem">
Axelsson, Ö. (2015). How to measure soundscape quality. In <em>Proceedings of euronoise 2015 :</em> (pp. 1477–1481). Stockholm University, Perception; psychophysics; Nederlands Akoestisch Genootschap; ABAV - Belgian Acoustical Society.
</div>
<div id="ref-Axelsson2010principal" class="csl-entry" role="listitem">
Axelsson, Ö., Nilsson, M. E., &amp; Berglund, B. (2010). <span class="nocase">A principal components model of soundscape perception</span>. <em>The Journal of the Acoustical Society of America</em>, <em>128</em>(5), 2836–2846. <a href="https://doi.org/10.1121/1.3493436">https://doi.org/10.1121/1.3493436</a>
</div>
<div id="ref-Axelsson2012Swedish" class="csl-entry" role="listitem">
Axelsson, Östen, Nilsson, M. E., &amp; Berglund, B. (2012). The <span>S</span>wedish soundscape-quality protocol. In <em>The Journal of the Acoustical Society of America</em> (Vol. 131, pp. 3476–3476). Acoustical Society of America (<span>ASA</span>). <a href="https://doi.org/10.1121/1.4709112">https://doi.org/10.1121/1.4709112</a>
</div>
<div id="ref-Azzalini2005Skew" class="csl-entry" role="listitem">
Azzalini, Adelchi. (2005). The skew-normal distribution and related multivariate families. <em>Scandinavian Journal of Statistics</em>, <em>32</em>(2), 159–188. <a href="https://doi.org/10.1111/j.1467-9469.2005.00426.x">https://doi.org/10.1111/j.1467-9469.2005.00426.x</a>
</div>
<div id="ref-Azzalini1999Statistical" class="csl-entry" role="listitem">
Azzalini, A., &amp; Capitanio, A. (1999). Statistical applications of the multivariate skew normal distribution. <em>Journal of the Royal Statistical Society Series B: Statistical Methodology</em>, <em>61</em>(3), 579–602. <a href="https://doi.org/10.1111/1467-9868.00194">https://doi.org/10.1111/1467-9868.00194</a>
</div>
<div id="ref-Azzalini1996Multivariate" class="csl-entry" role="listitem">
Azzalini, A., &amp; Valle, A. D. (1996). The multivariate skew-normal distribution. <em>Biometrika</em>, <em>83</em>(4), 715–726. Retrieved from <a href="http://www.jstor.org/stable/2337278">http://www.jstor.org/stable/2337278</a>
</div>
<div id="ref-Berglund2006Soundscape" class="csl-entry" role="listitem">
Berglund, B., &amp; Nilsson, M. E. (2006). <span class="nocase">Soundscape quality in suburban green areas and city parks</span>. <em>Acta Acustica United with Acustica</em>, <em>92</em>(6), 903–911.
</div>
<div id="ref-Blauert1997Sound" class="csl-entry" role="listitem">
Blauert, J., &amp; Jekosch, U. (1997). Sound-quality evaluation a multi-layered problem. <em>Acta Acustica United with Acustica</em>, <em>83</em>(5), 747–753. Retrieved from <a href="https://www.ingentaconnect.com/content/dav/aaua/1997/00000083/00000005/art00005">https://www.ingentaconnect.com/content/dav/aaua/1997/00000083/00000005/art00005</a>
</div>
<div id="ref-Chen2023Developing" class="csl-entry" role="listitem">
Chen, X., Aletta, F., Moshona, C., Henze, H., Mitchell, A., Oberman, T., et al. (2023). Developing a taxonomy of soundscape design from real-world examples. In <em>184th meeting of the acoustical society of america</em> (Vol. 153, pp. A232–A232). Chicago: Acoustical Society of America. <a href="https://doi.org/10.1121/10.0018743">https://doi.org/10.1121/10.0018743</a>
</div>
<div id="ref-Erfanian2019Psychophysiological" class="csl-entry" role="listitem">
Erfanian, M., Mitchell, A. J., Kang, J., &amp; Aletta, F. (2019). <span class="nocase">The Psychophysiological Implications of Soundscape: A Systematic Review of Empirical Literature and a Research Agenda</span>. <em>International Journal of Environmental Research and Public Health</em>, <em>16</em>(19), 3533. <a href="https://doi.org/10.3390/ijerph16193533">https://doi.org/10.3390/ijerph16193533</a>
</div>
<div id="ref-EuropeanUnion2002Directive" class="csl-entry" role="listitem">
European Union. (2002). <em><span class="nocase">Directive 2002/49/EC of the European Parliament and of the Council of 25 June 2002 relating to the assessment and management of environmental noise</span></em> (pp. 12–25).
</div>
<div id="ref-Fasano1987multidimensional" class="csl-entry" role="listitem">
Fasano, G., &amp; Franceschini, A. (1987). A multidimensional version of the kolmogorov–smirnov test. <em>Monthly Notices of the Royal Astronomical Society</em>, <em>225</em>(1), 155–170. <a href="https://doi.org/10.1093/mnras/225.1.155">https://doi.org/10.1093/mnras/225.1.155</a>
</div>
<div id="ref-Fastl2006Psychoacoustic" class="csl-entry" role="listitem">
Fastl, H. (2006). Psychoacoustic basis of sound quality evaluation and sound engineering. In <em>The thirteenth international congress on sound and vibration</em>. Vienna.
</div>
<div id="ref-Fiebig2018Does" class="csl-entry" role="listitem">
Fiebig, A. (2018). <span class="nocase">Does it make a difference to have soundscape standards ?</span> <em>Proceedings - Euronoise 2018</em>, (June), 6. Retrieved from <a href="https://www.euronoise2018.eu/docs/papers/482_Euronoise2018.pdf">https://www.euronoise2018.eu/docs/papers/482_Euronoise2018.pdf</a>
</div>
<div id="ref-Fletcher1933Loudness" class="csl-entry" role="listitem">
Fletcher, H., &amp; Munson, W. A. (1933). Loudness, its definition, measurement and calculation*. <em>Bell System Technical Journal</em>, <em>12</em>(4), 377–430. <a href="https://doi.org/10.1002/j.1538-7305.1933.tb00403.x">https://doi.org/10.1002/j.1538-7305.1933.tb00403.x</a>
</div>
<div id="ref-Hellman1987Why" class="csl-entry" role="listitem">
Hellman, R., &amp; Zwicker, E. (1987). Why can a decrease in dB(a) produce an increase in loudness? <em>The Journal of the Acoustical Society of America</em>, <em>82</em>(5), 1700–1705. <a href="https://doi.org/10.1121/1.395162">https://doi.org/10.1121/1.395162</a>
</div>
<div id="ref-ISO12913Part2" class="csl-entry" role="listitem">
ISO/TS 12913-2:2018. (2018). <span>Acoustics</span> – <span>Soundscape</span> – <span>Part</span> 2: <span>Data</span> collection and reporting requirements.
</div>
<div id="ref-Kang2018Impact" class="csl-entry" role="listitem">
Kang, J., &amp; Aletta, F. (2018). <span class="nocase">The Impact and Outreach of Soundscape Research</span>. <em>Environments</em>, <em>5</em>(5), 58. <a href="https://doi.org/10.3390/environments5050058">https://doi.org/10.3390/environments5050058</a>
</div>
<div id="ref-Kang2019Towards" class="csl-entry" role="listitem">
Kang, J., Aletta, F., Oberman, T., Erfanian, M., Kachlicka, M., Lionello, M., &amp; Mitchell, A. (2019). <span class="nocase">Towards soundscape indices</span>. In <em>Proceedings of the 23rd international congress on acoustics</em> (Vol. integrating 4th EAA Euroregio 2019 : 9–13 September 2019, pp. 2488–2495). Aachen: RWTH Aachen University. <a href="https://doi.org/10.18154/RWTH-CONV-239249">https://doi.org/10.18154/RWTH-CONV-239249</a>
</div>
<div id="ref-Kruize2019Exploring" class="csl-entry" role="listitem">
Kruize, H., Van Kamp, I., Van Den Berg, M., Van Kempen, E., Wendel-vos, W., Ruijsbroek, A., et al. (2019). <span class="nocase">Exploring mechanisms underlying the relationship between the natural outdoor environment and health and well-being – Results from the <span>PHENOTYPE</span> project</span>. <em>Environment International</em>, (October 2018), 105173. <a href="https://doi.org/10.1016/j.envint.2019.105173">https://doi.org/10.1016/j.envint.2019.105173</a>
</div>
<div id="ref-Kryter1970Effects" class="csl-entry" role="listitem">
Kryter, K. D. (1970). <em>The effects of noise on man</em>. (D. H. K. Lee, E. W. Hewson, &amp; C. F. Gurnham, Eds.). Burlington: Elsevier Science.
</div>
<div id="ref-Mitchell2022Predictive" class="csl-entry" role="listitem">
Mitchell, A. (2022, September). <em>Predictive <span>M</span>odelling of <span>C</span>omplex <span>U</span>rban <span>S</span>oundscapes: <span>E</span>nabling an engineering approach to soundscape design</em> (PhD Thesis). University College London. <a href="https://doi.org/10.13140/RG.2.2.15590.50245">https://doi.org/10.13140/RG.2.2.15590.50245</a>
</div>
<div id="ref-Mitchell2023Testing" class="csl-entry" role="listitem">
Mitchell, A., &amp; Aletta, F. (2023). Testing and adjusting soundscape circumplex translations. <em>OSF Preprints</em>. Preprint. <a href="https://doi.org/10.17605/OSF.IO/JVNA2">https://doi.org/10.17605/OSF.IO/JVNA2</a>
</div>
<div id="ref-Mitchell2020Soundscape" class="csl-entry" role="listitem">
Mitchell, A., Oberman, T., Aletta, F., Erfanian, M., Kachlicka, M., Lionello, M., &amp; Kang, J. (2020). <span class="nocase">The Soundscape Indices (SSID) Protocol: A Method for Urban Soundscape Surveys–Questionnaires with Acoustical and Contextual Information</span>. <em>Applied Sciences</em>, <em>10</em>(7), 2397. <a href="https://doi.org/10.3390/app10072397">https://doi.org/10.3390/app10072397</a>
</div>
<div id="ref-Mitchell2022How" class="csl-entry" role="listitem">
Mitchell, A., Aletta, F., &amp; Kang, J. (2022). How to analyse and represent quantitative soundscape data. <em>JASA Express Letters</em>, <em>2</em>(3), 037201. <a href="https://doi.org/10.1121/10.0009794">https://doi.org/10.1121/10.0009794</a>
</div>
<div id="ref-Parmanen2007weighted" class="csl-entry" role="listitem">
Parmanen, J. (2007). A-weighted sound pressure level as a loudness/annoyance indicator for environmental sounds – could it be improved? <em>Applied Acoustics</em>, <em>68</em>(1), 58–70. <a href="https://doi.org/10.1016/j.apacoust.2006.02.004">https://doi.org/10.1016/j.apacoust.2006.02.004</a>
</div>
<div id="ref-Russell1980circumplex" class="csl-entry" role="listitem">
Russell, J. A. (1980). A circumplex model of affect. <em>Journal of Personality and Social Psychology</em>, <em>39</em>(6), 1161. <a href="https://doi.org/10.1037/h0077714">https://doi.org/10.1037/h0077714</a>
</div>
<div id="ref-Yang2005Acoustic" class="csl-entry" role="listitem">
Yang, W., &amp; Kang, J. (2005). Acoustic comfort evaluation in urban open public spaces. <em>Applied Acoustics</em>, <em>66</em>(2), 211–229. <a href="https://doi.org/10.1016/j.apacoust.2004.07.011">https://doi.org/10.1016/j.apacoust.2004.07.011</a>
</div>
<div id="ref-Zwicker2007Psychoacoustics" class="csl-entry" role="listitem">
Zwicker, E., &amp; Fastl, H. (2007). <em><span class="nocase">Psychoacoustics: facts and models</span></em> (Third ed., p. 462). Berlin ; New York: Springer. <a href="https://doi.org/10.1007/978-3-540-68888-4">https://doi.org/10.1007/978-3-540-68888-4</a>
</div>
</div>
</section>
</section>

</main>
<!-- /main column -->
<script id = "quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<script>var lightboxQuarto = GLightbox({"loop":false,"descPosition":"bottom","closeEffect":"zoom","openEffect":"zoom","selector":".lightbox"});
window.onload = () => {
  lightboxQuarto.on('slide_before_load', (data) => {
    const { slideIndex, slideNode, slideConfig, player, trigger } = data;
    const href = trigger.getAttribute('href');
    if (href !== null) {
      const imgEl = window.document.querySelector(`a[href="${href}"] img`);
      if (imgEl !== null) {
        const srcAttr = imgEl.getAttribute("src");
        if (srcAttr && srcAttr.startsWith("data:")) {
          slideConfig.href = srcAttr;
        }
      }
    } 
  });

  lightboxQuarto.on('slide_after_load', (data) => {
    const { slideIndex, slideNode, slideConfig, player, trigger } = data;
    if (window.Quarto?.typesetMath) {
      window.Quarto.typesetMath(slideNode);
    }
  });

};
          </script>

</body>

</html>
